---
title: "SL-HW2"
output:
  html_document:
    highlight: kate
    toc: yes
---

```{r Install libraries, eval = FALSE}
renv::restore()

# or

renv::install(
  c(
    "data.table",
    "kernlab",
    "doFuture",
    "tidymodels",
    "readr",
    "RColorBrewer",
    "ggplot2",
    "plotly"
  )
)
```

```{r Load libraries}
suppressMessages(require(data.table, quietly = T))

suppressMessages(require(kernlab, quietly = T))

suppressMessages(require(parallel, quietly = T))
suppressMessages(require(doFuture, quietly = T))

suppressMessages(require(tidymodels, quietly = T))
suppressMessages(require(readr, quietly = T))

suppressMessages(require(RColorBrewer, quietly = T))
suppressMessages(require(ggplot2, quietly = T))
suppressMessages(require(plotly, quietly = T))
```

Load csv file and exclude the $m=10$ observations for `Part 2`

```{r Load csv}
data <- data.table::fread("data/train4final_hw.csv")

set.seed(123)
obs_to_be_removed <- sample(1:nrow(data), 10)

data <- data[-obs_to_be_removed ,]
```

Select only the columns we will use (all except `id`, `prec.x`, and all `time` related summaries) and thus define the starting dataset

```{r}
data_to_model <- data %>%
  as_tibble() %>%
  select(-starts_with("time"), -id, -prec.x) %>%
  mutate(genre = as.character(genre))
```

Parallel computations

```{r Parallel initialization}
num_cores <- parallel::detectCores() - 1
registerDoFuture()
```

# Part 1

## Model selection

We will go for SVM with Radial Basis Function, tuning the `cost` and `rbf_sigma` hyperparameters with k-Fold cross validation.

```{r Model selection}
svm_rbf_spec <- svm_rbf(cost = tune(), 
                        rbf_sigma = tune()) %>%
  set_mode("regression") %>%
  set_engine("kernlab")
```

## Exploratory Modelling (for us)

Steps

1.  Split data in 70% Training and 30% Testing with stratified sampling

2.  Grid search to find best parameters using k-Fold Cross Validation

3.  Fit with the best parameters and compute RMSE

### Training/testing

```{r Training/testing split}
set.seed(123)
tr_te_split <- data_to_model %>% initial_split(prop = 0.7, strata = tempo)

training <- training(tr_te_split)
testing <- testing(tr_te_split)
```

Our approach for dimensionality reduction:

- Exclude all `time` columns
- Linear PCA on all MEL coefficients columns
- Kernel PCA with RBF for all frequency and signal related columns
- One-hot encoding of `genre`

```{r Preprocessing}
svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
  step_normalize(starts_with("mel")) %>% 
  step_pca(starts_with("mel"), num_comp = 20) %>%
  step_kpca_rbf(starts_with("domfr"), starts_with("freq"),
                mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh,
                roughness,  rugo, sfm.1, shannon, simpson, renyi,
                num_comp = 5, sigma = 0.3) %>%
  step_dummy(genre, one_hot = T)
```

Define the workflow

```{r Workflow}
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(svm_rbf_rec)
```

### Tuning

```{r Tuning on 70% Training}
# parallel computing
cl <- makeCluster(num_cores)
plan(cluster, workers = cl)

# construct a grid of values
# of cost and sigma to check
svm_rbf_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)

# folds for cross validation
set.seed(123)
folds <- vfold_cv(training, v = 5, strata = tempo)

# fit for each set of folds
# for all parameters in the grid
svm_rbf_res <- svm_rbf_wf %>%
  tune_grid(resamples = folds, grid = svm_rbf_grid)
```

### Compare the parameters

```{r Compare tuned parameters on 70% training}
svm_rbf_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(cost = factor(cost)) %>%
  ggplot(aes(rbf_sigma, mean, color = cost)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Fit with the best parameters

```{r Best 70-30 Fit}
best_svm <- svm_rbf_res %>% select_best("rmse")

svm_rbf_wf_final <- svm_rbf_wf %>% finalize_workflow(best_svm)

svm_rbf_fit <- svm_rbf_wf_final %>% last_fit(tr_te_split) 
stopCluster(cl)
```

### RMSE on Test

```{r RMSE on Test}
svm_rbf_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>%
  select(.estimate) %>%
  deframe %>% 
  cat("RMSE =", .)
```

### Predictions vs Observations

```{r Predictions vs tempo}
testing_rows <- svm_rbf_fit$.predictions[[1]]$.row

svm_rbf_fit$.predictions[[1]] %>%
  ggplot() +
    aes(tempo, .pred) +
    geom_point(colour = "orangered", alpha = 0.3) + 
    geom_abline(slope = 1, intercept = 0, colour = "midnightblue")
```

# Final Modelling (for Kaggle)

```{r Training for Kaggle}
# train on full training dataset
training <- data_to_model

svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
  step_normalize(starts_with("mel")) %>%
  step_pca(starts_with("mel"), num_comp = 20) %>%
  step_kpca_rbf(starts_with("domfr"), starts_with("freq"),
                mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh,
                roughness,  rugo, sfm.1, shannon, simpson, renyi,
                num_comp = 5, sigma = 0.3) %>%
  step_dummy(genre, one_hot = T)

# workflow
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(svm_rbf_rec)

# parallel computing
cl <- makeCluster(num_cores)
plan(cluster, workers = cl)

# construct a 5x5 grid
# of cost and sigma to check
svm_rbf_grid <- grid_regular(cost(range = c(10, 23), trans = identity_trans()),
                             rbf_sigma(range = c(0.04, 0.06), trans = identity_trans()), levels = 5)

# folds for cross validation
set.seed(123)
folds <- vfold_cv(training, v = 10, strata = tempo)

# fit for each set of folds
# for all parameters in the grid
svm_rbf_res <- svm_rbf_wf %>%
  tune_grid(resamples = folds, grid = svm_rbf_grid)
```

### Compare the parameters

```{r Compare tuned parameters for Kaggle}
svm_rbf_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(cost = factor(cost)) %>%
  ggplot(aes(rbf_sigma, mean, color = cost)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Lowest RMSE from cross validation

```{r Parameters with lowest avg RMSE}
svm_rbf_res %>% show_best("rmse")
```


### Fit with the best parameters

```{r Final fit for Kaggle}
best_svm_rmse <- svm_rbf_res %>% select_best("rmse")
svm_rbf_fit_rmse <- svm_rbf_wf %>% 
  finalize_workflow(best_svm_rmse) %>%
  fit(training)

best_svm_std_err <- svm_rbf_res %>% select_by_one_std_err("rmse")
svm_rbf_fit_std_err <- svm_rbf_wf %>% 
  finalize_workflow(best_svm_std_err) %>%
  fit(training)

stopCluster(cl)
```

###  Save models

```{r Save model for Kaggle, eval = FALSE}
current_time <- Sys.time() %>% format("%Y%m%d_%H%M%S")

filename <- "models/svm_rbf" %>% 
  paste(current_time, "rmse", sep = "_") %>% 
  paste(".rds", sep = "")

svm_rbf_fit_rmse %>% 
  readr::write_rds(file = filename)

filename <- "models/svm_rbf" %>% 
  paste(current_time, "std_err", sep = "_") %>% 
  paste(".rds", sep = "")

svm_rbf_fit_std_err %>% 
  readr::write_rds(file = filename)
```

# Export Kaggle submission

### Import Test dataset

```{r}
testing_full <- data.table::fread("data/test4final_hw.csv")

testing <- testing_full  %>%
  as_tibble() %>%
  select(-starts_with("time"), -id, -prec.x) %>%
  mutate(genre = as.character(genre))
```

### Import saved model and export predictions

```{r}
model_date <- "20220605_142721"

for (type in c("rmse", "std_err")){
  svm_rbf_fit <- paste("models/svm_rbf_", model_date, "_", type, ".rds", sep = "") %>%
    readr::read_rds(.)
  
  target <- svm_rbf_fit %>% 
    predict(testing) %>% 
    select(target = .pred)

  final_df <- testing_full[, "id"] %>%
    as_tibble() %>% 
    bind_cols(target)
  
  paste("submissions/submission_", model_date, "_", type, ".csv", sep = "") %>% 
    write_csv(final_df, file = .)
}
```

# Part 2

