---
title: "SL-HW2"
output:
  html_document:
    highlight: kate
    toc: yes
---

```{r Install libraries, eval = FALSE}
renv::restore()

# or

renv::install(
  c(
    "data.table",
    "kernlab",
    "doFuture",
    "tidymodels",
    "readr",
    "RColorBrewer",
    "ggplot2",
    "plotly"
  )
)
```

```{r Load libraries}
suppressMessages(require(data.table, quietly = T))

suppressMessages(require(kernlab, quietly = T))

suppressMessages(require(parallel, quietly = T))
suppressMessages(require(doFuture, quietly = T))

suppressMessages(require(tidymodels, quietly = T))
suppressMessages(require(readr, quietly = T))

suppressMessages(require(RColorBrewer, quietly = T))
suppressMessages(require(ggplot2, quietly = T))
suppressMessages(require(plotly, quietly = T))
```

Load csv file and exclude the $m=10$ observations for `Part 2`

```{r Load csv}
data <- data.table::fread("data/train4final_hw.csv")

set.seed(123)
obs_to_be_removed <- sample(1:nrow(data), 10)

data <- data[-obs_to_be_removed ,]
```

Select only the columns we will use (all except `id`, `prec.x`, and all `time` related summaries) and thus define the starting dataset

```{r}
data_to_model <- data %>%
  as_tibble() %>%
  select(-starts_with("time"), -id, -prec.x) %>%
  mutate(genre = as.character(genre))
```

Parallel computations

```{r Parallel initialization}
num_cores <- parallel::detectCores() - 1
registerDoFuture()
```

# Part 1

## Model selection

We will go for SVM with Radial Basis Function, tuning the `cost` and `rbf_sigma` hyperparameters with k-Fold cross validation.

```{r Model selection}
svm_rbf_spec <- svm_rbf(cost = tune(), 
                        rbf_sigma = tune()) %>%
  set_mode("regression") %>%
  set_engine("kernlab")
```

## Exploratory Modelling (for us)

Steps

1.  Split data in 70% Training and 30% Testing with stratified sampling

2.  Grid search to find best parameters using k-Fold Cross Validation

3.  Fit with the best parameters and compute RMSE

### Training/testing

```{r Training/testing split}
set.seed(123)
tr_te_split <- data_to_model %>% initial_split(prop = 0.7, strata = tempo)

training <- training(tr_te_split)
testing <- testing(tr_te_split)
```

Our approach for dimensionality reduction:

- Exclude all `time` columns
- Linear PCA on all MEL coefficients columns
- Kernel PCA with RBF for all frequency and signal related columns
- One-hot encoding of `genre`

```{r Preprocessing}
svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
  step_pca(starts_with("mel"), num_comp = 20) %>%
  step_kpca_rbf(starts_with("domfr"), starts_with("freq"),
                mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh,
                roughness,  rugo, sfm.1, shannon, simpson, renyi,
                num_comp = 5, sigma = 0.3) %>%
  step_dummy(genre, one_hot = T)

# svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
#   step_pca(starts_with("mel"), num_comp = 20) %>%
#   step_kpca_rbf(-starts_with("mel"), -genre,
#                 num_comp = 5, sigma = 0.3) %>%
#   step_dummy(genre, one_hot = T)
```

Define the workflow

```{r Workflow}
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(svm_rbf_rec)
```

### Tuning

```{r Tuning on 70% Training}
# parallel computing
cl <- makeCluster(num_cores)
plan(cluster, workers = cl)

# construct a grid of values
# of cost and sigma to check
svm_rbf_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)

# folds for cross validation
set.seed(123)
folds <- vfold_cv(training, v = 5, strata = tempo)

# fit for each set of folds
# for all parameters in the grid
svm_rbf_res <- svm_rbf_wf %>%
  tune_grid(resamples = folds, grid = svm_rbf_grid)
```

### Compare the parameters

```{r Compare tuned parameters on 70% training}
svm_rbf_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(cost = factor(cost)) %>%
  ggplot(aes(rbf_sigma, mean, color = cost)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Fit with the best parameters

```{r Best 70-30 Fit}
best_svm <- svm_rbf_res %>% select_best("rmse")

svm_rbf_wf_final <- svm_rbf_wf %>% finalize_workflow(best_svm)

svm_rbf_fit <- svm_rbf_wf_final %>% last_fit(tr_te_split) 
stopCluster(cl)
```

### RMSE on Test

```{r RMSE on Test}
svm_rbf_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>%
  select(.estimate) %>%
  deframe %>% 
  cat("RMSE =", .)
```

### Predictions vs Observations

```{r Predictions vs tempo}
testing_rows <- svm_rbf_fit$.predictions[[1]]$.row

svm_rbf_fit$.predictions[[1]] %>%
  ggplot() +
    aes(tempo, .pred) +
    geom_point(colour = "orangered", alpha = 0.3) + 
    geom_abline(slope = 1, intercept = 0, colour = "midnightblue")
```

# Final Modelling (for Kaggle)

```{r Training for Kaggle}
# train on full training dataset
training <- data_to_model

# recipe
svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
  step_normalize(starts_with("mel")) %>% 
  step_pca(starts_with("mel"), num_comp = 20) %>%
  step_kpca_rbf(starts_with("domfr"), starts_with("freq"),
                mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh, 
                roughness,  rugo, sfm.1, shannon, simpson, renyi,
                num_comp = 5, sigma = 0.3) %>%
  step_dummy(genre, one_hot = T)

# workflow
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(svm_rbf_rec)

# parallel computing
cl <- makeCluster(num_cores)
plan(cluster, workers = cl)

# construct a 5x5 grid
# of cost and sigma to check
svm_rbf_grid <- grid_regular(cost(range = c(16, 23), trans = identity_trans()),
                             rbf_sigma(range = c(0.04, 0.06), trans = identity_trans()), levels = 5)
# svm_rbf_grid <- grid_regular(cost(),
#                              rbf_sigma(), levels = 10)

# 10 folds for cross validation
set.seed(123)
folds <- vfold_cv(training, v = 5, strata = tempo)

# fit for each set of folds
# for all parameters in the grid
svm_rbf_res <- svm_rbf_wf %>%
  tune_grid(resamples = folds, grid = svm_rbf_grid)
```

### Compare the parameters

```{r Compare tuned parameters for Kaggle}
svm_rbf_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(cost = factor(cost)) %>%
  ggplot(aes(rbf_sigma, mean, color = cost)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Lowest RMSE from cross validation

```{r Lowest RMSE from cross validation}
svm_rbf_res %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  filter(mean == min(mean)) %>%
  select(cost, rbf_sigma, .metric, mean, std_err)
```

### Fit with the best parameters

```{r Final fit for Kaggle}
best_svm <- svm_rbf_res %>% select_best("rmse")

svm_rbf_wf_final <- svm_rbf_wf %>% finalize_workflow(best_svm)

svm_rbf_fit <- svm_rbf_wf_final %>% fit(training)
stopCluster(cl)
```

###  Save model

```{r Save model for Kaggle, eval = FALSE}
current_time <- Sys.time() %>% format("%Y%m%d_%H%M%S")

filename <- "models/svm_rbf" %>% 
  paste(current_time, sep = "_") %>% 
  paste(".rds", sep = "")

svm_rbf_fit %>% 
  readr::write_rds(file = filename)
```

# Part 2

