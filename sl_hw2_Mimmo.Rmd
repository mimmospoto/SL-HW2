---
title: "SL-HW2"
output:
  html_document:
    highlight: kate
    toc: yes
---

```{r Install libraries, eval = FALSE}
renv::restore()

# or

renv::install(
  c(
    "data.table",
    "kernlab",
    "doFuture",
    "tidymodels",
    "readr",
    "RColorBrewer",
    "ggplot2",
    "plotly"
  )
)
```

```{r Load libraries}
suppressMessages(require(data.table, quietly = T))

suppressMessages(require(kernlab, quietly = T))

suppressMessages(require(parallel, quietly = T))
suppressMessages(require(doFuture, quietly = T))

suppressMessages(require(tidymodels, quietly = T))
suppressMessages(require(readr, quietly = T))

suppressMessages(require(RColorBrewer, quietly = T))
suppressMessages(require(ggplot2, quietly = T))
suppressMessages(require(plotly, quietly = T))
```

Load csv file and exclude the $m=10$ observations for `Part 2`

```{r Load csv}
data_import <- data.table::fread("/Users/domenicospoto/Desktop/Sapienza/MScDataScience/StatisticalLearning/HW2/data4final_hw/train4final_hw.csv")

set.seed(123)
obs_to_be_removed <- sample(1:nrow(data_import), 10)

data <- data_import[-obs_to_be_removed ,]
```

Select only the columns we will use (all except `id`, `prec.x`, and all `time` related summaries) and thus define the starting dataset

```{r}
data_to_model <- data %>%
  as_tibble() %>%
  select(-starts_with("time"), -id, -prec.x) %>%
  mutate(genre = as.character(genre))

```

Parallel computations

```{r Parallel initialization}
num_cores <- parallel::detectCores() - 1
registerDoFuture()
```

# Part 1

## Model selection

We will go for SVM with Radial Basis Function, tuning the `cost` and `rbf_sigma` hyperparameters with k-Fold cross validation.

```{r Model selection}
svm_rbf_spec <- svm_rbf(cost = tune(), 
                        rbf_sigma = tune()) %>%
  set_mode("regression") %>%
  set_engine("kernlab")
```

## Exploratory Modelling (for us)

Steps

1.  Split data in 70% Training and 30% Testing with stratified sampling

2.  Grid search to find best parameters using k-Fold Cross Validation

3.  Fit with the best parameters and compute RMSE

### Training/testing

```{r Training/testing split}
set.seed(123)
tr_te_split <- data_to_model %>% initial_split(prop = 0.7, strata = tempo)

training <- training(tr_te_split)
testing <- testing(tr_te_split)
```

Our approach for dimensionality reduction:

- Exclude all `time` columns
- Linear PCA on all MEL coefficients columns
- Kernel PCA with RBF for all frequency and signal related columns
- One-hot encoding of `genre`

```{r Preprocessing}
svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
  step_pca(starts_with("mel"), num_comp = 20) %>%
  step_kpca_rbf(starts_with("domfr"), 
                starts_with("freq"),
                mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh,
                roughness,  rugo, sfm.1, shannon, simpson, renyi,
                num_comp = 5, sigma = 0.3) %>%
  step_dummy(genre, one_hot = T)

# svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
#   step_pca(starts_with("mel"), num_comp = 20) %>%
#   step_kpca_rbf(-starts_with("mel"), -genre,
#                 num_comp = 5, sigma = 0.3) %>%
#   step_dummy(genre, one_hot = T)
```

Define the workflow

```{r Workflow}
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(svm_rbf_rec)
```

### Tuning

```{r Tuning on 70% Training}
# parallel computing
cl <- makeCluster(num_cores)
plan(cluster, workers = cl)

# construct a grid of values
# of cost and sigma to check
svm_rbf_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)

# folds for cross validation
folds <- vfold_cv(training, v = 5, strata = tempo)

# fit for each set of folds
# for all parameters in the grid
svm_rbf_res <- svm_rbf_wf %>%
  tune_grid(resamples = folds, grid = svm_rbf_grid)
```

### Compare the parameters

```{r Compare tuned parameters on 70% training}
svm_rbf_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(cost = factor(cost)) %>%
  ggplot(aes(rbf_sigma, mean, color = cost)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Fit with the best parameters

```{r Best 70-30 Fit}
best_svm <- svm_rbf_res %>% select_best("rmse")

svm_rbf_wf_final <- svm_rbf_wf %>% finalize_workflow(best_svm)

svm_rbf_fit <- svm_rbf_wf_final %>% last_fit(tr_te_split) 
stopCluster(cl)
```

### RMSE on Test

```{r RMSE on Test}
svm_rbf_fit %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>%
  select(.estimate) %>%
  deframe %>% 
  cat("RMSE =", .)
```

### Predictions vs Observations

```{r Predictions vs tempo}
testing_rows <- svm_rbf_fit$.predictions[[1]]$.row

svm_rbf_fit$.predictions[[1]] %>%
  ggplot() +
    aes(tempo, .pred) +
    geom_point(colour = "orangered", alpha = 0.3) + 
    geom_abline(slope = 1, intercept = 0, colour = "midnightblue")
```

# Final Modelling (for Kaggle)

```{r Training for Kaggle}
# train on full training dataset
training <- data_to_model

# recipe
svm_rbf_rec <- recipe(tempo ~ ., data = training) %>%
  step_pca(starts_with("mel"), num_comp = 20) %>%
  step_kpca_rbf(starts_with("domfr"), starts_with("freq"),
                mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh, 
                roughness,  rugo, sfm.1, shannon, simpson, renyi,
                num_comp = 5, sigma = 0.3) %>%
  step_dummy(genre, one_hot = T)

# workflow
svm_rbf_wf <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(svm_rbf_rec)

# parallel computing
cl <- makeCluster(num_cores)
plan(cluster, workers = cl)

# construct a 5x5 grid
# of cost and sigma to check
svm_rbf_grid <- grid_regular(cost(range = c(16, 23), trans = identity_trans()),
                             rbf_sigma(range = c(0.04, 0.06), trans = identity_trans()), levels = 5)
# svm_rbf_grid <- grid_regular(cost(),
#                              rbf_sigma(), levels = 10)

# 10 folds for cross validation
folds <- vfold_cv(training, v = 5, strata = tempo)

# fit for each set of folds
# for all parameters in the grid
svm_rbf_res <- svm_rbf_wf %>%
  tune_grid(resamples = folds, grid = svm_rbf_grid)
```

### Compare the parameters

```{r Compare tuned parameters for Kaggle}
svm_rbf_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  mutate(cost = factor(cost)) %>%
  ggplot(aes(rbf_sigma, mean, color = cost)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

### Lowest RMSE from cross validation

```{r Lowest RMSE from cross validation}
svm_rbf_res %>% 
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  filter(mean == min(mean)) %>%
  select(cost, rbf_sigma, .metric, mean, std_err)
```

### Fit with the best parameters

```{r Final fit for Kaggle}
best_svm <- svm_rbf_res %>% select_best("rmse")

svm_rbf_wf_final <- svm_rbf_wf %>% finalize_workflow(best_svm)

svm_rbf_fit <- svm_rbf_wf_final %>% fit(training) 
stopCluster(cl)
```

###  Save model

```{r Save model for Kaggle, eval = FALSE}
current_time <- Sys.time() %>% format("%Y%m%d_%H%M%S")

filename <- "/Users/domenicospoto/Desktop/Sapienza/MScDataScience/StatisticalLearning/HW2/SL-HW2/models" %>% 
  paste(current_time, sep = "_") %>% 
  paste(".rds", sep = "")

svm_rbf_fit %>% 
  readr::write_rds(file = filename)
```

# Part B
# Point 1
## Start from the best model from Part A
```{r}
train_model <- function(train){
  # train on half training dataset
  # Model selection
  svm_rbf_spec <- svm_rbf(cost = tune(), 
                        rbf_sigma = tune()) %>%
    set_mode("regression") %>%
    set_engine("kernlab")
  # recipe
  svm_rbf_rec <- recipe(tempo ~ ., data = train) %>%
    step_pca(starts_with("mel"), num_comp = 20) %>%
    step_kpca_rbf(starts_with("domfr"), starts_with("freq"),
                  mean, sd, sem, median, mode, Q25, Q75, IQR, cent, skewness, kurtosis, sfm, sh, 
                  roughness,  rugo, sfm.1, shannon, simpson, renyi,
                  num_comp = 5, sigma = 0.3) %>%
    step_dummy(genre, one_hot = T)
  
  # workflow
  svm_rbf_wf <- workflow() %>%
    add_model(svm_rbf_spec) %>%
    add_recipe(svm_rbf_rec)
  
  # parallel computing
  cl <- makeCluster(num_cores)
  plan(cluster, workers = cl)
  
  # construct a 5x5 grid
  # of cost and sigma to check
  # svm_rbf_grid <- grid_regular(cost(range = c(16, 23), trans = identity_trans()),
  #                              rbf_sigma(range = c(0.04, 0.06), trans = identity_trans()), levels =   5)
  svm_rbf_grid <- grid_regular(cost(),
                               rbf_sigma(), levels = 5)
  
  # 10 folds for cross validation
  folds <- vfold_cv(training, v = 5, strata = tempo)
  
  # fit for each set of folds
  # for all parameters in the grid
  svm_rbf_res <- svm_rbf_wf %>%
    tune_grid(resamples = folds, grid = svm_rbf_grid)
  
  # Final fit
  best_svm <- svm_rbf_res %>% select_best("rmse")

  svm_rbf_wf_final <- svm_rbf_wf %>% finalize_workflow(best_svm)

  svm_rbf_fit <- svm_rbf_wf_final %>% fit(training) 
  stopCluster(cl)
  return(svm_rbf_fit)
}
```

## Implement Split Conformal Prediction
# Import and create Xnew
```{r Create Xnew}
m <- data_import[obs_to_be_removed ,]
Xnew <- m %>%
  as_tibble() %>%
  select(-starts_with("time"), -id, -prec.x) %>%
  mutate(genre = as.character(genre))
```

# Tune the model on D1

```{r tune D1}
set.seed(123)
D <- data_to_model
idx <- sample(1:nrow(D), floor(nrow(D)/2))
D1 <- D[idx,]
D2 <- D[-idx,]
svm_rbf_fit_CP <- train_model(D1)
```

# Save train on D1

```{r save D1}
filename <- "/Users/domenicospoto/Desktop/Sapienza/MScDataScience/StatisticalLearning/HW2/SL-HW2/models" %>% 
  paste('/train_D1_CP', sep='') %>% 
  paste(".rds", sep = "")
svm_rbf_fit_CP %>% 
  readr::write_rds(file = filename)
```

# Compute absolute value of Y and residuals on D2
```{r}
resOut <- abs(D2$tempo - predict(svm_rbf_fit_CP, new_data=D2))[[1]]
```

# Build C.split
```{r}
alpha <- 0.025
kOut <- ceiling(((nrow(D)/2)+1)*(1-alpha))
resUse <- resOut[order(resOut)][kOut]
Y.hat <- predict(svm_rbf_fit_CP, new_data=Xnew)[[1]]
C.split <- matrix(c(Y.hat-resUse,Y.hat+resUse, m$tempo), nrow = 10, ncol = 3)
colnames(C.split) <- c('LowerCP', 'UpperCP', 'True.response')
```

# Plot and visualize the results of alpha=0.025
```{r Plot visualization of the results for m=10}
C.split %>%
  as_tibble() %>%
  ggplot(aes(True.response, Y.hat))+
  geom_abline(slope = 1, alpha = 0.3, lty=2)+
  geom_ribbon(aes(ymin=LowerCP,ymax=UpperCP), alpha=0.08, fill='midnightblue')+
  geom_errorbar(aes(ymin=LowerCP,ymax=UpperCP))+
  geom_point(color='#F8766D', size=2.5)
```

# Plot and visualize results for various alpha and width intervals
```{r compute error rate and width int}
alpha <- 0.1*1:10
build_intervals <- function(alpha){
  kOut <- ceiling(((nrow(D)/2)+1)*(1-alpha))
  resUse <- resOut[order(resOut)][kOut]
  Y.hat <- predict(svm_rbf_fit_CP, new_data=Xnew)[[1]]
  C.split <- matrix(c(Y.hat-resUse,Y.hat+resUse, m$tempo), nrow = 10, ncol = 3)
  colnames(C.split) <- c('LowerCP', 'UpperCP', 'True.response')
  error_rate <- length(which(C.split[,'True.response'] < C.split[,'LowerCP']  | C.split[,'True.response'] > C.split[,'UpperCP']))/nrow(C.split)
  width_interval <- mean(C.split[,'UpperCP']-C.split[,'LowerCP'])
  return (list('C.split'=C.split, 'error_rate'=error_rate, 'width_interval'=width_interval))
}

C.values <- matrix(nrow=10,ncol=3)
C.values[,1] <- alpha
for (i in 1:length(alpha)){
 out <- build_intervals(alpha[i])
 C.values[i,2] <- out$error_rate
 C.values[i,3] <- out$width_interval
}
colnames(C.values) <- c('alpha', 'error_rate', 'width_interval')
```

```{r}
C.values %>%
  as_tibble()%>%
  ggplot(aes(alpha,error_rate))+
  geom_abline(slope = 1, alpha = 0.3, lty=2)+
  geom_point()+
  scale_y_continuous(sec.axis = sec_axis(~ . + 10))

  

geom_line(aes(y=width_interval), lty=2)

```


# COMMENTS !!!!

# Point 2

```{r}
data_test <- data.table::fread("/Users/domenicospoto/Desktop/Sapienza/MScDataScience/StatisticalLearning/HW2/data4final_hw/test4final_hw.csv")

set.seed(123)
obs_to_be_considered <- sample(1:nrow(data_test), 100)
data_test <- data_test[obs_to_be_considered, ]

Xnew_test <- data_test %>%
  as_tibble() %>%
  select(-starts_with("time"), -id, -prec.x) %>%
  mutate(genre = as.character(genre))
Xnew_test$genre

```


# Build C.split
```{r}
alpha <- 0.025
kOut <- ceiling(((nrow(D)/2)+1)*(1-alpha))
resUse <- resOut[order(resOut)][kOut]
Y.hat <- predict(svm_rbf_fit_CP, new_data=Xnew_test)[[1]]
C.split_test <- matrix(c(Y.hat-resUse,Y.hat+resUse, Y.hat), nrow = 100, ncol = 3)
colnames(C.split_test) <- c('LowerCP', 'UpperCP', 'Y.hat')
```

# Plot and visualize the results of alpha=0.025
```{r Plot visualization of the results for m=10}
C.split_test %>%
  as_tibble() %>%
  ggplot(aes(x=1:100))+
  geom_ribbon(aes(ymin=LowerCP,ymax=UpperCP), alpha=0.2, fill='midnightblue')+
  geom_line(aes(y=Y.hat))+
  geom_point(aes(y=Y.hat),color='#F8766D', size=2)+
  xlab('x')
```

